{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Copy of mnist_with_keras.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sankirnajoshi/intro-to-dl/blob/master/week2/v2/mnist_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhY7zlcDchoI",
        "colab_type": "text"
      },
      "source": [
        "# MNIST digits classification with Keras\n",
        "\n",
        "We don't expect you to code anything here because you've already solved it with TensorFlow.\n",
        "\n",
        "But you can appreciate how simpler it is with Keras.\n",
        "\n",
        "We'll be happy if you play around with the architecture though, there're some tips at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN0AG2qFchoQ",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://github.com/sankirnajoshi/intro-to-dl/blob/master/week2/v2/images/mnist_sample.png?raw=1\" style=\"width:30%\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUO9tGIgczMG",
        "colab_type": "code",
        "outputId": "86eebc4a-87d0-4f35-b873-fb5eaf8d854a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "! shred -u setup_google_colab.py\n",
        "! wget https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py -O setup_google_colab.py\n",
        "import setup_google_colab\n",
        "# please, uncomment the week you're working on\n",
        "# setup_google_colab.setup_week1()\n",
        "setup_google_colab.setup_week2()\n",
        "# setup_google_colab.setup_week3()\n",
        "# setup_google_colab.setup_week4()\n",
        "# setup_google_colab.setup_week5()\n",
        "# setup_google_colab.setup_week6()\n",
        "\n",
        "# If you're using the old version of the course (check a path of notebook on Coursera, you'll see v1 or v2),\n",
        "# use setup_week2_old()."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shred: setup_google_colab.py: failed to open for writing: No such file or directory\n",
            "--2019-12-01 01:15:17--  https://raw.githubusercontent.com/hse-aml/intro-to-dl/master/setup_google_colab.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3636 (3.6K) [text/plain]\n",
            "Saving to: ‘setup_google_colab.py’\n",
            "\n",
            "setup_google_colab. 100%[===================>]   3.55K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-01 01:15:17 (101 MB/s) - ‘setup_google_colab.py’ saved [3636/3636]\n",
            "\n",
            "**************************************************\n",
            "inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "**************************************************\n",
            "cifar-10-batches-py.tar.gz\n",
            "**************************************************\n",
            "mnist.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E0Az4itchoT",
        "colab_type": "code",
        "outputId": "e0b0d30e-c180-4bcc-dbe2-255205cf1cad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "print(\"We're using TF\", tf.__version__)\n",
        "import keras\n",
        "print(\"We are using Keras\", keras.__version__)\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../..\")\n",
        "import keras_utils\n",
        "from keras_utils import reset_tf_session"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "We're using TF 1.15.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "We are using Keras 2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcOTqqoJchob",
        "colab_type": "text"
      },
      "source": [
        "# Look at the data\n",
        "\n",
        "In this task we have 50000 28x28 images of digits from 0 to 9.\n",
        "We will train a classifier on this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6pGEbeGchoe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "447be824-9517-4122-dd78-6597ce128af4"
      },
      "source": [
        "import preprocessed_mnist\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = preprocessed_mnist.load_dataset()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "10682368/11490434 [==========================>...] - ETA: 0s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9BSECIJchol",
        "colab_type": "code",
        "outputId": "a01e5b76-2a4d-40cc-a3df-a5c8c163a1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "# X contains rgb values divided by 255\n",
        "print(\"X_train [shape %s] sample patch:\\n\" % (str(X_train.shape)), X_train[1, 15:20, 5:10])\n",
        "print(\"A closeup of a sample patch:\")\n",
        "plt.imshow(X_train[1, 15:20, 5:10], cmap=\"Greys\")\n",
        "plt.show()\n",
        "print(\"And the whole sample:\")\n",
        "plt.imshow(X_train[1], cmap=\"Greys\")\n",
        "plt.show()\n",
        "print(\"y_train [shape %s] 10 samples:\\n\" % (str(y_train.shape)), y_train[:10])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train [shape (50000, 28, 28)] sample patch:\n",
            " [[0.         0.29803922 0.96470588 0.98823529 0.43921569]\n",
            " [0.         0.33333333 0.98823529 0.90196078 0.09803922]\n",
            " [0.         0.33333333 0.98823529 0.8745098  0.        ]\n",
            " [0.         0.33333333 0.98823529 0.56862745 0.        ]\n",
            " [0.         0.3372549  0.99215686 0.88235294 0.        ]]\n",
            "A closeup of a sample patch:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "And the whole sample:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAOdUlEQVR4nO3dfayU5ZnH8d8lLb4AEpAjQXvicRET\ntYnQTMgmJQ2bug3oH0h8CUQJa4g0BJSa+haMqTGayLotSlyJsBBw7dI0FCN/mLVKGrF/2DgClRez\nq4sH4QQ5hwip1Wh5ufaP89gc8Tz3HGaemWfg+n6Sycw819znuTL645l57pm5zd0F4Nx3XtkNAGgN\nwg4EQdiBIAg7EARhB4L4Tit3Nm7cOO/q6mrlLoFQuru7deTIERus1lDYzWyGpGclDZP0H+7+VOrx\nXV1dqlarjewSQEKlUsmt1f0y3syGSfp3STMlXStprpldW+/fA9BcjbxnnyrpQ3ff5+5/k/QbSbOK\naQtA0RoJ++WSDgy4fzDb9g1mttDMqmZW7evra2B3ABrR9LPx7r7a3SvuXuno6Gj27gDkaCTsPZI6\nB9z/XrYNQBtqJOzvSJpkZlea2XBJcyRtKaYtAEWre+rN3U+Y2RJJr6l/6m2du+8prDMAhWpont3d\nX5X0akG9AGgiPi4LBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ\nhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBAtXbIZ554DBw4k688++2xubcWKFcmx9913X7K+\ndOnSZL2zszNZj4YjOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTw7knp6epL1KVOmJOvHjh3LrZlZ\ncuwzzzyTrG/YsCFZ7+vrS9ajaSjsZtYt6TNJJyWdcPdKEU0BKF4RR/Z/cvcjBfwdAE3Ee3YgiEbD\n7pJ+b2bvmtnCwR5gZgvNrGpmVd5DAeVpNOzT3P0HkmZKWmxmPzr9Ae6+2t0r7l7p6OhocHcA6tVQ\n2N29J7vulfSypKlFNAWgeHWH3cxGmNmor29L+omk3UU1BqBYjZyNHy/p5Wyu9DuS/svd/7uQrtAy\n+/fvT9anT5+erB89ejRZT82ljx49Ojn2/PPPT9Z7e3uT9X379uXWrrjiiuTYYcOGJetno7rD7u77\nJF1fYC8AmoipNyAIwg4EQdiBIAg7EARhB4LgK67ngOPHj+fWak2tzZgxI1mv9VPRjZg8eXKy/uST\nTybr06ZNS9YnTZqUW1u9enVy7IIFC5L1sxFHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0Ignn2c8AD\nDzyQW3vuueda2MmZefPNN5P1zz//PFmfPXt2sr558+bc2o4dO5Jjz0Uc2YEgCDsQBGEHgiDsQBCE\nHQiCsANBEHYgCObZzwK1vlP+0ksv5dbcvaF915rLvuWWW5L1O++8M7fW2dmZHHvNNdck6w899FCy\nvmnTptxao8/L2YgjOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EYa2cb6xUKl6tVlu2v7NFT09Psn79\n9enFco8dO1b3vu+4445kfc2aNcn63r17k/Xt27fn1ubMmZMce9FFFyXrtaSWXR4xYkRy7J49e5L1\nWp8RKEulUlG1Wh10neyaR3YzW2dmvWa2e8C2sWb2upl9kF2PKbJhAMUbysv49ZJOXzbkYUlb3X2S\npK3ZfQBtrGbY3X2bpE9P2zxL0obs9gZJNxfcF4CC1XuCbry7H8pufyJpfN4DzWyhmVXNrNrX11fn\n7gA0quGz8d5/hi/3LJ+7r3b3irtXOjo6Gt0dgDrVG/bDZjZBkrLr3uJaAtAM9YZ9i6T52e35kl4p\nph0AzVLz++xmtlHSdEnjzOygpF9IekrSb81sgaT9km5vZpNnuyNHjiTry5cvT9aPHj2arI8fn3vK\nRFdeeWVy7KJFi5L14cOHJ+u11livVS/LF198kaw//fTTyfrKlSuLbKclaobd3efmlH5ccC8AmoiP\nywJBEHYgCMIOBEHYgSAIOxAEPyVdgBMnTiTr999/f7Ke+iloSRo9enSy/tprr+XWrrrqquTY48eP\nJ+tRffTRR2W3UDiO7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBPPsBfj444+T9Vrz6LW8/fbbyfrV\nV19d99++8MIL6x6LswtHdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0Ignn2AixevDhZr7Us9uzZs5P1\nRubRIzt16lRu7bzz0se5Vi5l3ioc2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCObZh2jHjh25tW3b\ntiXHmlmyftttt9XVE9JSc+m1/ptUKpWi2yldzSO7ma0zs14z2z1g22Nm1mNmO7PLjc1tE0CjhvIy\nfr2kGYNsX+Huk7PLq8W2BaBoNcPu7tskfdqCXgA0USMn6JaY2XvZy/wxeQ8ys4VmVjWzal9fXwO7\nA9CIesO+StJESZMlHZL0y7wHuvtqd6+4e6Wjo6PO3QFoVF1hd/fD7n7S3U9JWiNparFtAShaXWE3\nswkD7s6WtDvvsQDaQ815djPbKGm6pHFmdlDSLyRNN7PJklxSt6SfNrHHtvDll1/m1r766qvk2Msu\nuyxZv+mmm+rq6VxXa937lStX1v23b7311mR92bJldf/tdlUz7O4+d5DNa5vQC4Am4uOyQBCEHQiC\nsANBEHYgCMIOBMFXXFvgggsuSNZHjhzZok7aS62ptVWrViXrDz74YLLe1dWVW3vkkUeSY4cPH56s\nn404sgNBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEMyzt8C8efPKbqE0PT09ubXly5cnxz7//PPJ+l13\n3ZWsr1mzJlmPhiM7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgTBPPsQuXtdNUlav359sv7oo4/W01Jb\n2LhxY7J+zz335NaOHj2aHHvvvfcm6ytWrEjW8U0c2YEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCObZ\nh8jM6qpJ0sGDB5P1xx9/PFlfsGBBsj5q1Kjc2p49e5JjX3jhhWT9rbfeSta7u7uT9YkTJ+bW5syZ\nkxxba54dZ6bmkd3MOs3sD2a218z2mNnSbPtYM3vdzD7Irsc0v10A9RrKy/gTkn7u7tdK+kdJi83s\nWkkPS9rq7pMkbc3uA2hTNcPu7ofcfXt2+zNJ70u6XNIsSRuyh22QdHOzmgTQuDM6QWdmXZKmSPqT\npPHufigrfSJpfM6YhWZWNbNqX19fA60CaMSQw25mIyX9TtLP3P0vA2ve/02QQb8N4u6r3b3i7pWO\njo6GmgVQvyGF3cy+q/6g/9rdN2ebD5vZhKw+QVJvc1oEUISaU2/WP6+0VtL77v6rAaUtkuZLeiq7\nfqUpHZ4DTp48mazXmnpbu3Ztsj527Njc2q5du5JjGzVz5sxkfcaMGbm1JUuWFN0OEoYyz/5DSfMk\n7TKzndm2ZeoP+W/NbIGk/ZJub06LAIpQM+zu/kdJeZ8a+XGx7QBoFj4uCwRB2IEgCDsQBGEHgiDs\nQBB8xXWIrrvuutzaDTfckBz7xhtvNLTvWl+RTS2LXMull16arC9atChZP5t/BjsajuxAEIQdCIKw\nA0EQdiAIwg4EQdiBIAg7EATz7EN08cUX59Y2bdqUHPviiy8m6838yeQnnngiWb/77ruT9UsuuaTI\ndlAijuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EIT1L+bSGpVKxavVasv2B0RTqVRUrVYH/TVojuxA\nEIQdCIKwA0EQdiAIwg4EQdiBIAg7EETNsJtZp5n9wcz2mtkeM1uabX/MzHrMbGd2ubH57QKo11B+\nvOKEpJ+7+3YzGyXpXTN7PautcPd/a157AIoylPXZD0k6lN3+zMzel3R5sxsDUKwzes9uZl2Spkj6\nU7ZpiZm9Z2brzGxMzpiFZlY1s2pfX19DzQKo35DDbmYjJf1O0s/c/S+SVkmaKGmy+o/8vxxsnLuv\ndveKu1c6OjoKaBlAPYYUdjP7rvqD/mt33yxJ7n7Y3U+6+ylJayRNbV6bABo1lLPxJmmtpPfd/VcD\ntk8Y8LDZknYX3x6AogzlbPwPJc2TtMvMdmbblkmaa2aTJbmkbkk/bUqHAAoxlLPxf5Q02PdjXy2+\nHQDNwifogCAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQbR0\nyWYz65O0f8CmcZKOtKyBM9OuvbVrXxK91avI3q5w90F//62lYf/Wzs2q7l4prYGEdu2tXfuS6K1e\nreqNl/FAEIQdCKLssK8uef8p7dpbu/Yl0Vu9WtJbqe/ZAbRO2Ud2AC1C2IEgSgm7mc0ws/8xsw/N\n7OEyeshjZt1mtitbhrpaci/rzKzXzHYP2DbWzF43sw+y60HX2Cupt7ZYxjuxzHipz13Zy5+3/D27\nmQ2T9L+S/lnSQUnvSJrr7ntb2kgOM+uWVHH30j+AYWY/kvRXSS+6+/ezbf8q6VN3fyr7h3KMuz/U\nJr09JumvZS/jna1WNGHgMuOSbpb0LyrxuUv0dbta8LyVcWSfKulDd9/n7n+T9BtJs0roo+25+zZJ\nn562eZakDdntDer/n6XlcnprC+5+yN23Z7c/k/T1MuOlPneJvlqijLBfLunAgPsH1V7rvbuk35vZ\nu2a2sOxmBjHe3Q9ltz+RNL7MZgZRcxnvVjptmfG2ee7qWf68UZyg+7Zp7v4DSTMlLc5errYl738P\n1k5zp0NaxrtVBllm/O/KfO7qXf68UWWEvUdS54D738u2tQV378mueyW9rPZbivrw1yvoZte9Jffz\nd+20jPdgy4yrDZ67Mpc/LyPs70iaZGZXmtlwSXMkbSmhj28xsxHZiROZ2QhJP1H7LUW9RdL87PZ8\nSa+U2Ms3tMsy3nnLjKvk56705c/dveUXSTeq/4z8/0l6pIwecvr6B0l/zi57yu5N0kb1v6w7rv5z\nGwskXSJpq6QPJL0haWwb9fafknZJek/9wZpQUm/T1P8S/T1JO7PLjWU/d4m+WvK88XFZIAhO0AFB\nEHYgCMIOBEHYgSAIOxAEYQeCIOxAEP8PJdJc1jCDmVwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "y_train [shape (50000,)] 10 samples:\n",
            " [5 0 4 1 9 2 1 3 1 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnnNu4n_chor",
        "colab_type": "code",
        "outputId": "ca25a6d0-93fe-4a06-db60-8a62cc1d3d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# flatten images\n",
        "X_train_flat = X_train.reshape((X_train.shape[0], -1))\n",
        "print(X_train_flat.shape)\n",
        "\n",
        "X_val_flat = X_val.reshape((X_val.shape[0], -1))\n",
        "print(X_val_flat.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGBXUA_vchoy",
        "colab_type": "code",
        "outputId": "b1f15e56-0481-4ef0-a0a5-0184111933ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# one-hot encode the target\n",
        "y_train_oh = keras.utils.to_categorical(y_train, 10)\n",
        "y_val_oh = keras.utils.to_categorical(y_val, 10)\n",
        "\n",
        "print(y_train_oh.shape)\n",
        "print(y_train_oh[:3], y_train[:3])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 10)\n",
            "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]] [5 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN5pjEgNcho5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "8074c5de-7303-44ef-cf08-6efbe37b65b9"
      },
      "source": [
        "# building a model with keras\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        "# we still need to clear a graph though\n",
        "s = reset_tf_session()\n",
        "\n",
        "model = Sequential()  # it is a feed-forward network without loops like in RNN\n",
        "model.add(Dense(256, input_shape=(784,)))  # the first layer must specify the input shape (replacing placeholders)\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/keras_utils.py:68: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:79: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:82: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:84: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/keras_utils.py:75: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/keras_utils.py:77: The name tf.InteractiveSession is deprecated. Please use tf.compat.v1.InteractiveSession instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3535: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ6L-M4ycho_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "f4a72d43-8412-4f0e-9219-1264ccf67cb4"
      },
      "source": [
        "# you can look at all layers and parameter count\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 269,322\n",
            "Trainable params: 269,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiPb8yBKchpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "dbb37a1a-4609-4e53-edfd-b829a2cd0398"
      },
      "source": [
        "# now we \"compile\" the model specifying the loss and optimizer\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy', # this is our cross-entropy\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']  # report accuracy during training\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:697: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2745: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2749: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdAzyZHichpN",
        "colab_type": "code",
        "outputId": "1fa3d2c4-3298-4dd1-ce4b-ecb2109a18cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# and now we can fit the model with model.fit()\n",
        "# and we don't have to write loops and batching manually as in TensorFlow\n",
        "model.fit(\n",
        "    X_train_flat, \n",
        "    y_train_oh,\n",
        "    batch_size=512, \n",
        "    epochs=40,\n",
        "    validation_data=(X_val_flat, y_val_oh),\n",
        "    callbacks=[keras_utils.TqdmProgressCallback()],\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2289: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:879: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:602: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:866: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "\n",
            "Epoch 1/40\n",
            "Epoch 1/40\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:333: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:341: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "48128/50000 [===========================>..] - ETA: 0s - loss: 1.2100 - acc: 0.6981**\n",
            "loss: 1.1818; acc: 0.7053; val_loss: 0.4504; val_acc: 0.8901\n",
            "50000/50000 [==============================] - 3s - loss: 1.1839 - acc: 0.7047 - val_loss: 0.4504 - val_acc: 0.8901\n",
            "\n",
            "Epoch 2/40\n",
            "Epoch 2/40\n",
            "48128/50000 [===========================>..] - ETA: 0s - loss: 0.3822 - acc: 0.8978**\n",
            "loss: 0.3799; acc: 0.8981; val_loss: 0.2907; val_acc: 0.9174\n",
            "50000/50000 [==============================] - 0s - loss: 0.3802 - acc: 0.8980 - val_loss: 0.2907 - val_acc: 0.9174\n",
            "\n",
            "Epoch 3/40\n",
            "Epoch 3/40\n",
            "49152/50000 [============================>.] - ETA: 0s - loss: 0.2895 - acc: 0.9171*\n",
            "loss: 0.2896; acc: 0.9171; val_loss: 0.2441; val_acc: 0.9301\n",
            "50000/50000 [==============================] - 0s - loss: 0.2896 - acc: 0.9170 - val_loss: 0.2441 - val_acc: 0.9301\n",
            "\n",
            "Epoch 4/40\n",
            "Epoch 4/40\n",
            "49152/50000 [============================>.] - ETA: 0s - loss: 0.2473 - acc: 0.9281*\n",
            "loss: 0.2472; acc: 0.9280; val_loss: 0.2168; val_acc: 0.9383\n",
            "50000/50000 [==============================] - 0s - loss: 0.2473 - acc: 0.9280 - val_loss: 0.2168 - val_acc: 0.9383\n",
            "\n",
            "Epoch 5/40\n",
            "Epoch 5/40\n",
            "46080/50000 [==========================>...] - ETA: 0s - loss: 0.2181 - acc: 0.9356****\n",
            "loss: 0.2183; acc: 0.9357; val_loss: 0.1936; val_acc: 0.9463\n",
            "50000/50000 [==============================] - 0s - loss: 0.2184 - acc: 0.9357 - val_loss: 0.1936 - val_acc: 0.9463\n",
            "\n",
            "Epoch 6/40\n",
            "Epoch 6/40\n",
            "48640/50000 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9427**\n",
            "loss: 0.1940; acc: 0.9428; val_loss: 0.1802; val_acc: 0.9496\n",
            "50000/50000 [==============================] - 0s - loss: 0.1941 - acc: 0.9428 - val_loss: 0.1802 - val_acc: 0.9496\n",
            "\n",
            "Epoch 7/40\n",
            "Epoch 7/40\n",
            "49152/50000 [============================>.] - ETA: 0s - loss: 0.1739 - acc: 0.9490*\n",
            "loss: 0.1736; acc: 0.9491; val_loss: 0.1639; val_acc: 0.9546\n",
            "50000/50000 [==============================] - 0s - loss: 0.1736 - acc: 0.9491 - val_loss: 0.1639 - val_acc: 0.9546\n",
            "\n",
            "Epoch 8/40\n",
            "Epoch 8/40\n",
            "46080/50000 [==========================>...] - ETA: 0s - loss: 0.1549 - acc: 0.9541****\n",
            "loss: 0.1560; acc: 0.9540; val_loss: 0.1528; val_acc: 0.9551\n",
            "50000/50000 [==============================] - 0s - loss: 0.1556 - acc: 0.9541 - val_loss: 0.1528 - val_acc: 0.9551\n",
            "\n",
            "Epoch 9/40\n",
            "Epoch 9/40\n",
            "45568/50000 [==========================>...] - ETA: 0s - loss: 0.1417 - acc: 0.9585*****\n",
            "loss: 0.1407; acc: 0.9587; val_loss: 0.1390; val_acc: 0.9601\n",
            "50000/50000 [==============================] - 0s - loss: 0.1405 - acc: 0.9587 - val_loss: 0.1390 - val_acc: 0.9601\n",
            "\n",
            "Epoch 10/40\n",
            "Epoch 10/40\n",
            "49152/50000 [============================>.] - ETA: 0s - loss: 0.1280 - acc: 0.9625*\n",
            "loss: 0.1280; acc: 0.9626; val_loss: 0.1321; val_acc: 0.9618\n",
            "50000/50000 [==============================] - 0s - loss: 0.1280 - acc: 0.9626 - val_loss: 0.1321 - val_acc: 0.9618\n",
            "\n",
            "Epoch 11/40\n",
            "Epoch 11/40\n",
            "47104/50000 [===========================>..] - ETA: 0s - loss: 0.1160 - acc: 0.9668***\n",
            "loss: 0.1157; acc: 0.9667; val_loss: 0.1243; val_acc: 0.9641\n",
            "50000/50000 [==============================] - 0s - loss: 0.1157 - acc: 0.9667 - val_loss: 0.1243 - val_acc: 0.9641\n",
            "\n",
            "Epoch 12/40\n",
            "Epoch 12/40\n",
            "48640/50000 [============================>.] - ETA: 0s - loss: 0.1069 - acc: 0.9692**\n",
            "loss: 0.1067; acc: 0.9693; val_loss: 0.1165; val_acc: 0.9665\n",
            "50000/50000 [==============================] - 0s - loss: 0.1067 - acc: 0.9693 - val_loss: 0.1165 - val_acc: 0.9665\n",
            "\n",
            "Epoch 13/40\n",
            "Epoch 13/40\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.0958 - acc: 0.9721*\n",
            "loss: 0.0956; acc: 0.9721; val_loss: 0.1100; val_acc: 0.9678\n",
            "50000/50000 [==============================] - 0s - loss: 0.0957 - acc: 0.9721 - val_loss: 0.1100 - val_acc: 0.9678\n",
            "\n",
            "Epoch 14/40\n",
            "Epoch 14/40\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.0876 - acc: 0.9751*\n",
            "loss: 0.0877; acc: 0.9750; val_loss: 0.1103; val_acc: 0.9685\n",
            "50000/50000 [==============================] - 0s - loss: 0.0877 - acc: 0.9751 - val_loss: 0.1103 - val_acc: 0.9685\n",
            "\n",
            "Epoch 15/40\n",
            "Epoch 15/40\n",
            "49152/50000 [============================>.] - ETA: 0s - loss: 0.0798 - acc: 0.9770*\n",
            "loss: 0.0799; acc: 0.9770; val_loss: 0.1010; val_acc: 0.9700\n",
            "50000/50000 [==============================] - 0s - loss: 0.0798 - acc: 0.9770 - val_loss: 0.1010 - val_acc: 0.9700\n",
            "\n",
            "Epoch 16/40\n",
            "Epoch 16/40\n",
            "46080/50000 [==========================>...] - ETA: 0s - loss: 0.0718 - acc: 0.9796****\n",
            "loss: 0.0718; acc: 0.9795; val_loss: 0.0994; val_acc: 0.9704\n",
            "50000/50000 [==============================] - 0s - loss: 0.0718 - acc: 0.9795 - val_loss: 0.0994 - val_acc: 0.9704\n",
            "\n",
            "Epoch 17/40\n",
            "Epoch 17/40\n",
            "49152/50000 [============================>.] - ETA: 0s - loss: 0.0670 - acc: 0.9808*\n",
            "loss: 0.0669; acc: 0.9808; val_loss: 0.0952; val_acc: 0.9715\n",
            "50000/50000 [==============================] - 0s - loss: 0.0669 - acc: 0.9808 - val_loss: 0.0952 - val_acc: 0.9715\n",
            "\n",
            "Epoch 18/40\n",
            "Epoch 18/40\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.0602 - acc: 0.9830*\n",
            "loss: 0.0602; acc: 0.9830; val_loss: 0.0919; val_acc: 0.9719\n",
            "50000/50000 [==============================] - 0s - loss: 0.0602 - acc: 0.9830 - val_loss: 0.0919 - val_acc: 0.9719\n",
            "\n",
            "Epoch 19/40\n",
            "Epoch 19/40\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9843*\n",
            "loss: 0.0540; acc: 0.9845; val_loss: 0.0890; val_acc: 0.9734\n",
            "50000/50000 [==============================] - 0s - loss: 0.0541 - acc: 0.9844 - val_loss: 0.0890 - val_acc: 0.9734\n",
            "\n",
            "Epoch 20/40\n",
            "Epoch 20/40\n",
            "45056/50000 [==========================>...] - ETA: 0s - loss: 0.0491 - acc: 0.9863*****\n",
            "loss: 0.0497; acc: 0.9862; val_loss: 0.0916; val_acc: 0.9710\n",
            "50000/50000 [==============================] - 0s - loss: 0.0496 - acc: 0.9862 - val_loss: 0.0916 - val_acc: 0.9710\n",
            "\n",
            "Epoch 21/40\n",
            "Epoch 21/40\n",
            "47104/50000 [===========================>..] - ETA: 0s - loss: 0.0451 - acc: 0.9878***\n",
            "loss: 0.0450; acc: 0.9878; val_loss: 0.0823; val_acc: 0.9756\n",
            "50000/50000 [==============================] - 0s - loss: 0.0450 - acc: 0.9877 - val_loss: 0.0823 - val_acc: 0.9756\n",
            "\n",
            "Epoch 22/40\n",
            "Epoch 22/40\n",
            "45568/50000 [==========================>...] - ETA: 0s - loss: 0.0398 - acc: 0.9896*****\n",
            "loss: 0.0401; acc: 0.9895; val_loss: 0.0845; val_acc: 0.9749\n",
            "50000/50000 [==============================] - 0s - loss: 0.0402 - acc: 0.9895 - val_loss: 0.0845 - val_acc: 0.9749\n",
            "\n",
            "Epoch 23/40\n",
            "Epoch 23/40\n",
            "48640/50000 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9904**\n",
            "loss: 0.0366; acc: 0.9906; val_loss: 0.0832; val_acc: 0.9738\n",
            "50000/50000 [==============================] - 0s - loss: 0.0367 - acc: 0.9906 - val_loss: 0.0832 - val_acc: 0.9738\n",
            "\n",
            "Epoch 24/40\n",
            "Epoch 24/40\n",
            "47104/50000 [===========================>..] - ETA: 0s - loss: 0.0342 - acc: 0.9912***\n",
            "loss: 0.0342; acc: 0.9911; val_loss: 0.0815; val_acc: 0.9763\n",
            "50000/50000 [==============================] - 0s - loss: 0.0342 - acc: 0.9912 - val_loss: 0.0815 - val_acc: 0.9763\n",
            "\n",
            "Epoch 25/40\n",
            "Epoch 25/40\n",
            "46080/50000 [==========================>...] - ETA: 0s - loss: 0.0297 - acc: 0.9930****\n",
            "loss: 0.0298; acc: 0.9929; val_loss: 0.0775; val_acc: 0.9759\n",
            "50000/50000 [==============================] - 0s - loss: 0.0298 - acc: 0.9930 - val_loss: 0.0775 - val_acc: 0.9759\n",
            "\n",
            "Epoch 26/40\n",
            "Epoch 26/40\n",
            "46080/50000 [==========================>...] - ETA: 0s - loss: 0.0271 - acc: 0.9940****\n",
            "loss: 0.0274; acc: 0.9940; val_loss: 0.0801; val_acc: 0.9751\n",
            "50000/50000 [==============================] - 0s - loss: 0.0274 - acc: 0.9940 - val_loss: 0.0801 - val_acc: 0.9751\n",
            "\n",
            "Epoch 27/40\n",
            "Epoch 27/40\n",
            "46080/50000 [==========================>...] - ETA: 0s - loss: 0.0245 - acc: 0.9947****\n",
            "loss: 0.0245; acc: 0.9946; val_loss: 0.0790; val_acc: 0.9766\n",
            "50000/50000 [==============================] - 0s - loss: 0.0245 - acc: 0.9946 - val_loss: 0.0790 - val_acc: 0.9766\n",
            "\n",
            "Epoch 28/40\n",
            "Epoch 28/40\n",
            "45568/50000 [==========================>...] - ETA: 0s - loss: 0.0225 - acc: 0.9954*****\n",
            "loss: 0.0223; acc: 0.9955; val_loss: 0.0770; val_acc: 0.9766\n",
            "50000/50000 [==============================] - 0s - loss: 0.0223 - acc: 0.9955 - val_loss: 0.0770 - val_acc: 0.9766\n",
            "\n",
            "Epoch 29/40\n",
            "Epoch 29/40\n",
            "49152/50000 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9961*\n",
            "loss: 0.0198; acc: 0.9961; val_loss: 0.0775; val_acc: 0.9779\n",
            "50000/50000 [==============================] - 0s - loss: 0.0199 - acc: 0.9961 - val_loss: 0.0775 - val_acc: 0.9779\n",
            "\n",
            "Epoch 30/40\n",
            "Epoch 30/40\n",
            "45568/50000 [==========================>...] - ETA: 0s - loss: 0.0171 - acc: 0.9971*****\n",
            "loss: 0.0173; acc: 0.9971; val_loss: 0.0776; val_acc: 0.9782\n",
            "50000/50000 [==============================] - 0s - loss: 0.0173 - acc: 0.9971 - val_loss: 0.0776 - val_acc: 0.9782\n",
            "\n",
            "Epoch 31/40\n",
            "Epoch 31/40\n",
            "48640/50000 [============================>.] - ETA: 0s - loss: 0.0161 - acc: 0.9971**\n",
            "loss: 0.0163; acc: 0.9970; val_loss: 0.0737; val_acc: 0.9780\n",
            "50000/50000 [==============================] - 0s - loss: 0.0162 - acc: 0.9970 - val_loss: 0.0737 - val_acc: 0.9780\n",
            "\n",
            "Epoch 32/40\n",
            "Epoch 32/40\n",
            "46080/50000 [==========================>...] - ETA: 0s - loss: 0.0146 - acc: 0.9975****\n",
            "loss: 0.0144; acc: 0.9976; val_loss: 0.0743; val_acc: 0.9787\n",
            "50000/50000 [==============================] - 0s - loss: 0.0143 - acc: 0.9976 - val_loss: 0.0743 - val_acc: 0.9787\n",
            "\n",
            "Epoch 33/40\n",
            "Epoch 33/40\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9979*\n",
            "loss: 0.0131; acc: 0.9980; val_loss: 0.0775; val_acc: 0.9780\n",
            "50000/50000 [==============================] - 0s - loss: 0.0131 - acc: 0.9980 - val_loss: 0.0775 - val_acc: 0.9780\n",
            "\n",
            "Epoch 34/40\n",
            "Epoch 34/40\n",
            "48128/50000 [===========================>..] - ETA: 0s - loss: 0.0119 - acc: 0.9983**\n",
            "loss: 0.0118; acc: 0.9984; val_loss: 0.0740; val_acc: 0.9787\n",
            "50000/50000 [==============================] - 0s - loss: 0.0118 - acc: 0.9984 - val_loss: 0.0740 - val_acc: 0.9787\n",
            "\n",
            "Epoch 35/40\n",
            "Epoch 35/40\n",
            "49664/50000 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9985*\n",
            "loss: 0.0105; acc: 0.9985; val_loss: 0.0741; val_acc: 0.9785\n",
            "50000/50000 [==============================] - 0s - loss: 0.0105 - acc: 0.9985 - val_loss: 0.0741 - val_acc: 0.9785\n",
            "\n",
            "Epoch 36/40\n",
            "Epoch 36/40\n",
            "49152/50000 [============================>.] - ETA: 0s - loss: 0.0097 - acc: 0.9987*\n",
            "loss: 0.0097; acc: 0.9987; val_loss: 0.0767; val_acc: 0.9778\n",
            "50000/50000 [==============================] - 0s - loss: 0.0097 - acc: 0.9987 - val_loss: 0.0767 - val_acc: 0.9778\n",
            "\n",
            "Epoch 37/40\n",
            "Epoch 37/40\n",
            "47104/50000 [===========================>..] - ETA: 0s - loss: 0.0083 - acc: 0.9990***\n",
            "loss: 0.0083; acc: 0.9990; val_loss: 0.0754; val_acc: 0.9796\n",
            "50000/50000 [==============================] - 0s - loss: 0.0084 - acc: 0.9990 - val_loss: 0.0754 - val_acc: 0.9796\n",
            "\n",
            "Epoch 38/40\n",
            "Epoch 38/40\n",
            "46080/50000 [==========================>...] - ETA: 0s - loss: 0.0074 - acc: 0.9993****\n",
            "loss: 0.0075; acc: 0.9992; val_loss: 0.0760; val_acc: 0.9787\n",
            "50000/50000 [==============================] - 0s - loss: 0.0075 - acc: 0.9992 - val_loss: 0.0760 - val_acc: 0.9787\n",
            "\n",
            "Epoch 39/40\n",
            "Epoch 39/40\n",
            "46592/50000 [==========================>...] - ETA: 0s - loss: 0.0067 - acc: 0.9995****\n",
            "loss: 0.0066; acc: 0.9995; val_loss: 0.0763; val_acc: 0.9781\n",
            "50000/50000 [==============================] - 0s - loss: 0.0066 - acc: 0.9995 - val_loss: 0.0763 - val_acc: 0.9781\n",
            "\n",
            "Epoch 40/40\n",
            "Epoch 40/40\n",
            "46080/50000 [==========================>...] - ETA: 0s - loss: 0.0058 - acc: 0.9995****\n",
            "loss: 0.0059; acc: 0.9995; val_loss: 0.0755; val_acc: 0.9792\n",
            "50000/50000 [==============================] - 0s - loss: 0.0059 - acc: 0.9995 - val_loss: 0.0755 - val_acc: 0.9792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4c85f87b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6alp4HaAchpU",
        "colab_type": "text"
      },
      "source": [
        "# Here're the notes for those who want to play around here\n",
        "\n",
        "Here are some tips on what you could do:\n",
        "\n",
        " * __Network size__\n",
        "   * More neurons, \n",
        "   * More layers, ([docs](https://keras.io/))\n",
        "\n",
        "   * Other nonlinearities in the hidden layers\n",
        "     * tanh, relu, leaky relu, etc\n",
        "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
        "\n",
        "\n",
        " * __Early Stopping__\n",
        "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
        "   * Some networks converge over 5 epochs, others - over 500.\n",
        "   * Way to go: stop when validation score is 10 iterations past maximum\n",
        "     \n",
        "\n",
        " * __Faster optimization__\n",
        "   * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
        "     * Converge faster and sometimes reach better optima\n",
        "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
        "\n",
        "\n",
        " * __Regularize__ to prevent overfitting\n",
        "   * Add some L2 weight norm to the loss function, theano will do the rest\n",
        "     * Can be done manually or via - https://keras.io/regularizers/\n",
        "   \n",
        "   \n",
        " * __Data augmemntation__ - getting 5x as large dataset for free is a great deal\n",
        "   * https://keras.io/preprocessing/image/\n",
        "   * Zoom-in+slice = move\n",
        "   * Rotate+zoom(to remove black stripes)\n",
        "   * any other perturbations\n",
        "   * Simple way to do that (if you have PIL/Image): \n",
        "     * ```from scipy.misc import imrotate,imresize```\n",
        "     * and a few slicing\n",
        "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Bv5BznchpX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}