{"cells":[{"metadata":{},"cell_type":"markdown","source":"# A study of Unsupervised Learning techniques on the FIFA 20 dataset.\n![FIFA20](https://media.contentapi.ea.com/content/dam/ea/fifa/fifa-20/common/nav/fifa20-nav-clubpacks.png)"},{"metadata":{},"cell_type":"markdown","source":"In this project we explore the data through the domain of unsupervised learning performing principal component analysis and clustering analysis. One goal of this project is to best describe the variation in the different types of players. Doing so would equip us with insight into how to best choose players in a team. In a high-dimensional data, it is often difficult to develop an intuition of the features and our goal in this project is to reduce the dimensionality of the dataset so that we can visualize the relationships between the features and clusters in our dataset. We start with 104 features and bring down the dimensionality to 28 features by selecting key features using our domain knowledge, removing highly correlated features using regression techniques, and then further to just two principal components using PCA. We visualize the data using these principal components, perform clustering analysis and visualize the clusters and develop an inference for the same."},{"metadata":{},"cell_type":"markdown","source":"# Dataset\n\nFIFA 20 features more than 30 official leagues, over 700 clubs and over 17,000 players. Included for the first time is the Romanian Liga I and its 14 teams, as well as Emirati club Al Ain, who were added following extensive requests from the fans in the region.    \nWith the amount of player and team data available on the game, this makes for an interesting dataset with a rich in-depth breakdown of every possible recorded attribute a player can have. In addition, football lends a detailed structure to the player data due to the dynamic nature of the game and various positions a player may command. This results in a very complex and interesting structure in the dataset which we will look to explore in this project through unsupervised learning techniques"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML, display\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.set_option('display.max_columns', 500)\ndata = pd.read_csv('/kaggle/input/fifa-20-complete-player-dataset/players_20.csv') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(data.head())\nprint(f'The Data has {data.shape[0]} rows and {data.shape[1]} features')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Understanding and cleaning"},{"metadata":{},"cell_type":"markdown","source":"This dataset provides the complete statistics available at the player level in the FIFA 20 game. This dataset was scraped from the sofifa.com and is very clean in terms of the structure and expected data types. We will just transform the data to the format we need, and keep only selected features which might be useful in our analysis.    \nOur data has a lot of features that are actually explained or have the same information captured in them as other variables.\n\nSome key observations in the dataset:\n-\t`potential` and `overall` are highly correlated. `potential` is basically an integer greater than or equal to `overall`.\n-\t`overall` is a computation based on all the other skill ratings of a player such as `shooting`, `passing`, etc.\n-\tUnless a player plays at a Goalkeeper position (`GW`), all his goalkeeper statistics are `NaNs`.\n-\tThe columns `ls`, `st`, `rs`, `lw` etc. are playing positions in the game and the data in these columns is basically the max potential of a player if he were to play in that position. We will assume a player only plays in his preferred position and we will drop all these columns.\n-\tFor our analysis, we will drop all columns *unnecessary for our analysis as and when we reach that conclusion*. For now, all descriptive columns like `sofifa_id`, `player_url`, `nationality` etc. will be dropped.\n-\t`player_positions` are the preferred positions of the player. We will keep only the first playing position for our analysis.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"data['player_positions'] = data['player_positions'].str.split(',').str[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prior to dropping our columns as discussed above, lets take a  copy of the data. Since we are interested to see the clusters the data forms, it would be interesting to take some samples from various playing positions and see how they get transformed into our clusters.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data = data.copy()\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def generate_samples(positions = ['CAM', 'RM', 'CDM', 'LM', 'CM'], n_samples = 10):\n    '''\n    positions = ['RW', 'ST', 'LW', 'GK', 'CAM', 'CB', 'CM', 'CDM', 'CF', 'LB', 'RB','RM', 'LM', 'LWB', 'RWB']\n    '''\n    samples = original_data[original_data.player_positions.isin(positions) & (original_data.overall>=70)].sample(n_samples)\n    return samples.index.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets take a look at the player ratings"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax = sns.distplot(data.overall, bins=20);\nax.set_title('Distributions of Player Ratings')\nax.set_xlabel('Overall Ratings');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since, `overall` decides the overall quality of a player, we can plot its histogram to visualize how players are distributed. For the purpose of this notebook, we will remove players with `overall` rating lower than 70. This is just a soft criterion and a generally decent score on FIFA.\n\nHere we see an almost normal distribution of player age with their rankings."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data.overall>=70]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data.overall.value_counts().sort_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax = sns.distplot(data.overall, bins=20,vertical=False,kde=False);\nax.set_title('Distributions of Player Ratings')\nax.set_xlabel('Overall Ratings');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax = sns.scatterplot('age','overall',hue='player_positions',data=data);\nax.set_title('Player Ages vs Overall Rating')\nax.set_xlabel('Ages')\nax.set_ylabel('Overall Rating')\n\ndef label_point(x, y, val, ax):\n    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n    for i, point in a.iterrows():\n        if (point['y'] >=90) :\n            ax.text(point['x']+.1, point['y']+.1, str(point['val']),fontsize=12)\n\nlabel_point( data.age,data.overall, data.short_name, plt.gca())  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"1. We see an obvious yet interesting insight here that a player reaches his maximum potential during the middle of his career, usually in between 25-32 years of age. Since we suspect the data to be highly correlated, we will look at a heatmap of the correlations in the data below."},{"metadata":{"trusted":true},"cell_type":"code","source":"to_drop = ['sofifa_id','player_url','long_name','potential','dob',\\\n           'work_rate','body_type','real_face','release_clause_eur','player_tags',\\\n           'team_position','team_jersey_number','loaned_from','joined','contract_valid_until',\\\n           'nation_position','nation_jersey_number','player_traits',\\\n           'ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram','lm','lcm','cm','rcm',\\\n           'rm','lwb','ldm','cdm','rdm','rwb','lb','lcb','cb','rcb','rb', 'value_eur','wage_eur']\ndata = data.drop(to_drop, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.DataFrame(data.dtypes).T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets look at the correlation in the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create correlation matrix\ncorr_matrix = data.corr().abs()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nax = sns.heatmap(corr_matrix)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see, goalkeeper related features are perfectly correlated as seen by the white squares in the data. Goalkeepers are a separate group and none of the main player skills apply to goalkeepers. We will assume this as a separate cluster and remove all goalkeepers from the dataset. Now we will have to also drop the rows with player_position with the value GK. That is, we will also drop all the goalkeepers from the dataset. PCA requires continuous features only and hence we will also drop all features that are categorical. The reason for this is that PCA looks to capture the maximum variance in the data in the principal components and categorical features are discrete in nature with zero variance."},{"metadata":{"trusted":true},"cell_type":"code","source":"goalkeeper_features = ['gk_handling','gk_reflexes','gk_positioning','gk_diving','gk_kicking','gk_speed',\\\n                       'goalkeeping_diving','goalkeeping_handling','goalkeeping_kicking','goalkeeping_positioning','goalkeeping_reflexes']\ndata = data.drop(goalkeeper_features, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will have to also drop the rows with `player_position` with the value `GK`. That is, we will also drop all the goalkeepers from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data[data.player_positions !='GK']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's drop all the categorical features"},{"metadata":{"trusted":true},"cell_type":"code","source":"categorical_features = ['short_name','nationality','club','preferred_foot','player_positions','international_reputation','weak_foot','skill_moves']\ndata = data.drop(categorical_features, axis =1)\ndata = data.fillna(0)\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Relevance\nNow we are left with over `39` variables and we still need to check if our initial assumptions that \"`overall` and other summary skills are be explained by the other variables\".    \n\nA simple way to check this is to run a regression model on these features as the response and all other features as predictors.    \n\nLet us build a `DecisionTree` model to check this. We will create a function to perform this regression. The function will run regression with some feature as response and all other features as predictors. The `R2 scores` for response greater than `0.95` only is shown below. We will remove these features as the variance in these features can be explained by the remaining variables and they do not add a lot of further information to our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_features(data, feature, random_state, hc_feat):\n    new_data = data.drop(feature, axis = 1)\n    \n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(new_data, data[feature], test_size = 0.20, random_state = random_state)\n\n    from sklearn.tree import DecisionTreeRegressor\n    regressor = DecisionTreeRegressor(random_state=random_state)\n    regressor.fit(X_train, y_train)\n    y_pred = regressor.predict(X_test)\n\n    score =  regressor.score(X_test, y_test)\n    if score >= 0.95:\n        hc_feat.append(feature)\n        print(\"R2 Score for feature {} is {}\".format(feature, round(score,3) ))\n    else:\n        pass\n    return hc_feat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hc_feat = []\nfor key in data:\n    model_features(data, key, random_state=13263600,hc_feat=hc_feat)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we can see the features `pace`,`shooting`,`passing`,`dribbling`,`defending`,`physic` have very high R2 scores. These are the overall statistics of the players and are calculated using the other independent features.    \nAlso, we see that the features `attacking_finishing`, `skill_dribbling`, `movement_acceleration`,`movement_sprint_speed`,`defending_standing_tackle` have R2 scores over 0.95.\nFor these reasons I will drop these features as well for our analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop(hc_feat, axis = 1)\nprint(data.shape)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling\n\nSince, for PCA we need scaled data, we will transform the dataset by trying various scaling techniques. The original distribution of the dataset can be seen in the figure below."},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,6))\nfor col in data.columns:\n    sns.kdeplot(data[col], shade=True)\nplt.legend(loc='best');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%html\n<style>\n  table {margin-left: 0 !important;}\n</style>","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can notice that the features are on different scales and most of the features are slightly left-skewed. We try the following scaling techniques on the data and chose the one that results in the best explained variance by the first two principal components:    \n\n| Scaling Technique |\n|------|\n|   Log Scaling\t  |\n|   Standard Scaling  |\n|   Min Max Scaling  |\n|   Log normal Scaling  |"},{"metadata":{},"cell_type":"markdown","source":"## Transforming the data for PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\ndef pca_results(data, n_components=8):\n    #PCA model\n    pca = PCA(n_components=n_components, random_state=1).fit(data)\n    \n    #DataFrame creation\n    dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n    components = pd.DataFrame(np.round(pca.components_, 4), columns = list(data.columns))\n    components.index = dimensions\n\n    # PCA explained variance\n    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n    variance_ratios.index = dimensions\n\n    # Create a bar plot visualization\n    fig, ax = plt.subplots(figsize = (25,10))\n\n    # Plot the feature weights as a function of the components\n    components.plot(ax = ax, kind = 'bar');\n    ax.set_ylabel(\"Feature Weights\")\n    ax.set_xticklabels(dimensions, rotation=0)\n    plt.legend(loc='upper right')\n\n    # Display the explained variance ratios\n    for i, ev in enumerate(pca.explained_variance_ratio_):\n        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n%.4f\"%(ev))\n\n    # Return a concatenated DataFrame\n    df = pd.concat([variance_ratios, components], axis = 1)\n    print(f'Total Variance Explained by the first 2 dimensions: {df.iloc[:2,0].sum()}')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nlog_data = np.log(data)\n\nscaler = StandardScaler()\nminmax = MinMaxScaler()\n\nminmax.fit(data)\nscaler.fit(data)\n\nscaled_data = scaler.transform(data)\nscaled_data = pd.DataFrame(scaled_data, columns=data.columns)\n\nminmax_data = minmax.transform(data)\nminmax_data = pd.DataFrame(minmax_data, columns=data.columns)\n\nscaler.fit(log_data)\nlog_normal_data = scaler.transform(log_data)\nlog_normal_data = pd.DataFrame(log_normal_data, columns=data.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_transformed_data(data):\n    plt.figure(figsize=(20,6))\n    for col in data.columns:\n        sns.kdeplot(data[col], shade=True)\n    plt.legend(loc='best');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Data\nplot_transformed_data(data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Log\nplot_transformed_data(log_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_transformed_data(scaled_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_transformed_data(minmax_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_transformed_data(log_normal_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_log = pca_results(log_data, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_scaled = pca_results(scaled_data, 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_minmax = pca_results(minmax_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca_log_normal = pca_results(log_normal_data,5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scaling techniques and variance explained by the first two principal components are summarized in the table below:\n\n| Scaling Technique | Variance explained by PC1 and PC2 |\n|------|------|\n|   Log Scaling\t  |    68.4%    |\n|   Standard Scaling  |    52.5%    |\n|   Min Max Scaling  |    57.2%    |\n|   Log normal Scaling  |    50.8%    |\n"},{"metadata":{},"cell_type":"markdown","source":"# PCA Analysis\n\nWhen using principal component analysis, one of the main goals is to reduce the dimensionality of the data — in effect, reducing the complexity of the problem.    \nHowever, dimensionality reduction comes at a cost as fewer dimensions used implies less of the total variance in the data is being explained. Because of this, the cumulative explained variance ratio is extremely important for knowing how many dimensions are necessary for the problem. Additionally, if a significant amount of variance is explained by only two or three dimensions, the reduced data can be visualized afterwards.    \n\nSince PCs describe variation and account for the varied influences of the original characteristics, we can plot the PCs to find out which feature produces the differences among clusters.    \nTo do this we plot the loadings, or vectors representing each feature of the PC plot centered at (0, 0) with the direction and length of these vectors showing how much significance each feature has on the PCs. Also, the angle between these vectors let us know correlation between the features with a small angle denoting high correlation. A plot that visualizes the above information is called a Biplot.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_pca(data, sample_ids):\n    pca = PCA(n_components=2).fit(data)\n    reduced_data = pca.transform(log_data)\n    pca_samples = pca.transform(log_data[log_data.index.isin(sample_ids)])\n    reduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'])\n    return pca, reduced_data, pca_samples\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pca, reduced_data, pca_samples = make_pca(log_data, sample_ids = generate_samples(['CAM','CM']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def biplot(log_data, reduced_data, pca):\n    '''\n    Produce a biplot that shows a scatterplot of the reduced\n    data and the projections of the original features.\n    \n    good_data: original data, before transformation.\n               Needs to be a pandas dataframe with valid column names\n    reduced_data: the reduced data (the first two dimensions are plotted)\n    pca: pca object that contains the components_ attribute\n    return: a matplotlib AxesSubplot object (for any additional customization)\n    \n    This procedure is inspired by the script:\n    https://github.com/teddyroland/python-biplot\n    '''\n\n    fig, ax = plt.subplots(figsize = (10,10))\n    # scatterplot of the reduced data    \n    ax.scatter(x=reduced_data.loc[:, 'Dimension 1'], y=reduced_data.loc[:, 'Dimension 2'], \n        facecolors='b', edgecolors='b', s=70, alpha=0.5)\n    \n    feature_vectors = pca.components_.T\n\n    # we use scaling factors to make the arrows easier to see\n    arrow_size, text_pos = 4.0, 5.0,\n\n    # projections of the original features\n    for i, v in enumerate(feature_vectors):\n        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], \n                  head_width=0.2, head_length=0.2, linewidth=2, color='red')\n        ax.text(v[0]*text_pos, v[1]*text_pos, log_data.columns[i], color='black', \n                 ha='center', va='center', fontsize=18)\n\n    ax.set_xlabel(\"Dimension 1\", fontsize=14)\n    ax.set_ylabel(\"Dimension 2\", fontsize=14)\n    ax.set_title(\"PC plane with original feature projections.\", fontsize=16);\n    return ax\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The Biplot for our dataset is provided below. As we can see, the features mentality_interceptions, defending_sliding_tackle and defending_marking is close together. Also, these strongly influence both PC1 and PC2.  "},{"metadata":{"trusted":true},"cell_type":"code","source":"biplot(log_data, reduced_data, pca);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Clustering"},{"metadata":{},"cell_type":"markdown","source":"In this section, we choose to use a K-Means clustering algorithm to identify the various player segments hidden in the data.    \nAdvantages of KMeans clustering algorithm are: Kmeans is very fast. This is because Kmeans only needs to fit data to cluster centers. This makes KMeans faster in training.    \nHowever, one drawback is that KMeans only assigns hard clusters and does not give the probability score of the cluster.     \n\nBased on the data, it seems KMeans would do a good job assuming that the players are well segmented, and each player assumes a special role.    \n\nDepending on the problem, the number of clusters in the data may not be known in advance. As a result, we do not know for sure if a certain number of clusters are the best choice for our data. Since we do not know the structure present in the data, in order to measure the “goodness” of our clustering, we calculate each point’s **silhouette coefficient.**    \n\nThe silhouette coefficient for a data point measures how similar it is to its assigned cluster from -1 (dissimilar) to 1 (similar). Calculating the mean silhouette coefficient provides for a simple scoring method of a given clustering.    \n\nThe Silhouette Coefficient is defined for each sample and is composed of two scores (a and b):\n\na.\tThe mean distance between a sample and all other points in the same class.\nb.\tThe mean distance between a sample and all other points in the next nearest cluster.    \n\nThe Silhouette Coefficient s for a single sample is then given as:\n\n$$ s = \\frac{b-a}{max(a, b)}$$\n"},{"metadata":{},"cell_type":"markdown","source":"The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample. In our analysis, we receive the highest Silhouette Score of about `0.53` for three clusters. Another popular method to guess the appropriate number of clusters is the **Elbow Method**. In this method, we choose that value of `K`, which lies at the elbow of the curve plotted between the number of clusters and sum of distances between each point and its centroid. As we can see from the image below, the elbow of the curve appears at 3 clusters thus concurring with the Silhouette score."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import silhouette_score\nfrom sklearn.cluster import KMeans\n\ndef cluster(reduced_data,n_clusters,pca_samples=pca_samples):\n    clusterer = KMeans(n_clusters=n_clusters, random_state=123).fit(reduced_data)    \n    preds = clusterer.predict(reduced_data)\n    centers = clusterer.cluster_centers_\n    sample_preds = clusterer.predict(pca_samples)\n    return preds, centers, sample_preds\n\ndef silhouette_scorer(reduced_data,n_clusters):\n    preds,_,_ = cluster(reduced_data,n_clusters)\n    score = silhouette_score(reduced_data, preds)\n    return score\n\nfor n_clusters in range(2,10):\n    score = silhouette_scorer(reduced_data,n_clusters)\n    print (\"Silhoutte Score for {} cluster is {}\".format(n_clusters,score))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inertia = []\nclusters = range(2,10)\nfor n_clusters in clusters:\n    clusterer = KMeans(n_clusters=n_clusters, random_state=123).fit(reduced_data)\n    preds = clusterer.predict(reduced_data)\n    inertia.append(clusterer.inertia_)\n\nplt.plot(clusters, inertia)\nplt.ylabel('Inertia')\nplt.xlabel('n_clusters')\nplt.title('Elbow Method');\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we get three clusters"},{"metadata":{"trusted":true},"cell_type":"code","source":"def cluster_results(reduced_data, preds, centers, pca_samples):\n    '''\n    Visualizes the PCA-reduced cluster data in two dimensions\n    Adds cues for cluster centers and student-selected sample data\n    '''\n    import matplotlib.cm as cm\n    predictions = pd.DataFrame(preds, columns = ['Cluster'])\n    plot_data = pd.concat([predictions, reduced_data], axis = 1)\n\n    # Generate the cluster plot\n    fig, ax = plt.subplots(figsize = (14,8))\n\n    # Color map\n    cmap = cm.get_cmap('gist_rainbow')\n\n    # Color the points based on assigned cluster\n    for i, cluster in plot_data.groupby('Cluster'):   \n        cluster.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \\\n                     color = cmap((i)*1.0/(len(centers)-1)), label = 'Cluster %i'%(i), s=30);\n\n    # Plot centers with indicators\n    for i, c in enumerate(centers):\n        ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors = 'black', \\\n                   alpha = 1, linewidth = 2, marker = 'o', s=200);\n        ax.scatter(x = c[0], y = c[1], marker='$%d$'%(i), alpha = 1, s=100);\n\n    # Plot transformed sample points \n    ax.scatter(x = pca_samples[:,0], y = pca_samples[:,1], \\\n               s = 150, linewidth = 4, color = 'black', marker = 'x');\n\n    # Set plot title\n    ax.set_title(\"Cluster Learning on PCA-Reduced Data - Centroids Marked by Number\\nTransformed Sample Data Marked by Black Cross\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_data.player_positions.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://www.fifauteam.com/wp-content/uploads/2012/08/A046-1.jpg)"},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_ids = generate_samples(['CB','LB','RB'], 10)\n\n_, _, pca_samples = make_pca(log_data, sample_ids)\npreds, centers, sample_preds = cluster(reduced_data, 3)\ncluster_results(reduced_data, preds, centers, pca_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_ids = generate_samples(['ST','CF'], 10)\n\n_, _, pca_samples = make_pca(log_data, sample_ids)\npreds, centers, sample_preds = cluster(reduced_data, 3)\ncluster_results(reduced_data, preds, centers, pca_samples)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_ids = generate_samples(['CM','RM','LM','CAM','CDM'], 10)\n\n_, _, pca_samples = make_pca(log_data, sample_ids)\npreds, centers, sample_preds = cluster(reduced_data, 3)\ncluster_results(reduced_data, preds, centers, pca_samples)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Results\n\nIt appears that from the results above, the clusters are separated by the three main playing positions in the game.\n\n- Forwards (Green cluster or Cluster 1)\n- Midfielders (Pink cluster or Cluster 2)\n- Defenders (Red cluster or Cluster 0)\n\n## Interpretation\n\nIn order to understand why K Means returned the clusters that it returned, we should go back to the Biplot visualization above.    \nThe Biplot maps the original features as vectors to the principal components and comparing the clusters and the feature vectors, it becomes obvious.    \n\nFor example, lets take a look at Cluster 0, which is the Red cluster. We have inferred that its a cluster of defenders based on the random samples. Now, if we take a look at the Biplot, we can see that some of the features vectors have strong influence along the direction of this cluster is `defending_marking`, `mentality_interceptions`, `defending_sliding_tackle`.    \n\nSimilarly, we can observe that, the most important features in along cluster 2 are `skill_long_passing`, `short_passing`, `power_stamina`, `mental_composure` etc. Thus, we can infer that midfielders are the players that possess these traits and we can identify the strongest players in this cluster for a midfielder role.\n\nAnd finally, for a forward player, the main job is to score goals and naturally, the important features along this direction are Some of the key ones are `attacking`, `volleys`, `mentality_positioning`, `attacking_header_accuracy`. "},{"metadata":{},"cell_type":"markdown","source":"Important References\n\n1.\tFIFA 20 Information - Wikipedia.\" https://en.wikipedia.org/wiki/FIFA_20.\n2.\tFIFA 20 complete player dataset | Kaggle.\" 26 Sep. 2019, https://www.kaggle.com/stefanoleone992/fifa-20-complete-player-dataset\n3.\tSci-kit learn – Dimensionality Reduction - https://scikit-learn.org/stable/modules/decomposition.html\n4.\tSilhouette Score — scikit-learn 0.22.2 ....\" http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html.\n5.\tElbow method (clustering) - Wikipedia.\" https://en.wikipedia.org/wiki/Elbow_method_(clustering). \n6.\tUdacity – Clustering visualization https://github.com/udacity/mlnd\n7.\tHow to read PCA biplots and scree plots – Linh Ngo https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/\n8.\tSofifa.com – Data Scraped to Kaggle from sofifa -  sofifa.com \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}